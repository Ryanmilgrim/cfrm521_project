{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>CFRM 421/521, Spring 2023</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Ryan Milgrim</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Homework 4</center>\n",
    "</h1>\n",
    "\n",
    "* **Due: Wednesday, May 27, 2024, 11:59 PM**\n",
    "\n",
    "\n",
    "* Total marks: 43\n",
    "\n",
    "\n",
    "* Late submissions are allowed, but a 20% penalty per day applies. Your last submission is considered for calculating the penalty.\n",
    "\n",
    "\n",
    "*  Use this Jupyter notebook as a template for your solutions. **Your solution must be submitted as both one Jupyter notebook and one PDF file on Gradescope.** There will be two modules on Gradescope, one for each file type. The notebook must be already run, that is, make sure that you have run all the code, save the notebook, and then when you reopen the notebook, checked that all output appears as expected. You are allowed to use code from the textbook, textbook website, or lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A regression MLP [12 marks]\n",
    "\n",
    "Consider the original source of the California housing data (used in Homework 2) in Scikit-Learn.  The data is obtained and split using the code below, where we split off 20% as the test set, and then split off 20% of the training set as a validation set, and keep the remaining 80% of the training set as the actual training set. The following code creates the training set `X_train`, `y_train`, the validation set `X_valid`, `y_valid` and the test set `X_test`, `y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_9324\\4104529753.py:16: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "\n",
    "X_train_tmp, X_test, y_train_tmp, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_tmp, y_train_tmp, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [4 marks]\n",
    "\n",
    "Use `tensorflow.keras` to train a regression MLP with a normalization layer as the first layer (`tf.keras.layers.Normalization(input_shape=X_train.shape[1:])`), and one hidden layer of 50 ReLU neurons. For the output layer, try both a ReLU activation function and no activation function (which is equivalent to the identity function). Explain which choice is better. Use the appropriate weight initialization. Use the Nadam optimizer. Train for 30 epochs, and report the mean squared error on the validation set. In the `.compile()` method, use `loss=\"mse\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "<p>\n",
    "Model B performs better. I believe that model A suffers from an exploding gradient problem which is why every prediction on the validation set scores as a 0. \n",
    "<p\\>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\preprocessing\\normalization.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 5.5759 - val_loss: 6.3836\n",
      "Epoch 2/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7088 - val_loss: 6.3836\n",
      "Epoch 3/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5626 - val_loss: 6.3836\n",
      "Epoch 4/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6566 - val_loss: 6.3836\n",
      "Epoch 5/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 5.6240 - val_loss: 6.3836\n",
      "Epoch 6/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6065 - val_loss: 6.3836\n",
      "Epoch 7/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6135 - val_loss: 6.3836\n",
      "Epoch 8/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5476 - val_loss: 6.3836\n",
      "Epoch 9/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5878 - val_loss: 6.3836\n",
      "Epoch 10/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6056 - val_loss: 6.3836\n",
      "Epoch 11/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7159 - val_loss: 6.3836\n",
      "Epoch 12/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5943 - val_loss: 6.3836\n",
      "Epoch 13/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.4810 - val_loss: 6.3836\n",
      "Epoch 14/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6233 - val_loss: 6.3836\n",
      "Epoch 15/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6515 - val_loss: 6.3836\n",
      "Epoch 16/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5964 - val_loss: 6.3836\n",
      "Epoch 17/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5810 - val_loss: 6.3836\n",
      "Epoch 18/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5299 - val_loss: 6.3836\n",
      "Epoch 19/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5681 - val_loss: 6.3836\n",
      "Epoch 20/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5956 - val_loss: 6.3836\n",
      "Epoch 21/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6014 - val_loss: 6.3836\n",
      "Epoch 22/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6275 - val_loss: 6.3836\n",
      "Epoch 23/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5887 - val_loss: 6.3836\n",
      "Epoch 24/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6435 - val_loss: 6.3836\n",
      "Epoch 25/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6251 - val_loss: 6.3836\n",
      "Epoch 26/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7015 - val_loss: 6.3836\n",
      "Epoch 27/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.7166 - val_loss: 6.3836\n",
      "Epoch 28/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.6003 - val_loss: 6.3836\n",
      "Epoch 29/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5990 - val_loss: 6.3836\n",
      "Epoch 30/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5183 - val_loss: 6.3836\n",
      "Epoch 1/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 45392.1406 - val_loss: 2.1121\n",
      "Epoch 2/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.8260 - val_loss: 1.6019\n",
      "Epoch 3/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.6367 - val_loss: 1.6405\n",
      "Epoch 4/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.5115 - val_loss: 1.6751\n",
      "Epoch 5/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2125 - val_loss: 1.7056\n",
      "Epoch 6/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0257 - val_loss: 1.8573\n",
      "Epoch 7/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.9688 - val_loss: 1.4625\n",
      "Epoch 8/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0091 - val_loss: 1.1557\n",
      "Epoch 9/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.8522 - val_loss: 1.1459\n",
      "Epoch 10/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7989 - val_loss: 1.6357\n",
      "Epoch 11/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7590 - val_loss: 0.9409\n",
      "Epoch 12/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7359 - val_loss: 1.1640\n",
      "Epoch 13/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0246 - val_loss: 1.7845\n",
      "Epoch 14/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.7746 - val_loss: 3.0128\n",
      "Epoch 15/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 0.8322 - val_loss: 4.0022\n",
      "Epoch 16/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.2434 - val_loss: 1.8345\n",
      "Epoch 17/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.0475 - val_loss: 1.4237\n",
      "Epoch 18/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.4337 - val_loss: 1.0649\n",
      "Epoch 19/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 6.8641 - val_loss: 1.8734\n",
      "Epoch 20/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.1139 - val_loss: 2.5463\n",
      "Epoch 21/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 1.0554 - val_loss: 2.4131\n",
      "Epoch 22/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.6164 - val_loss: 1.3153\n",
      "Epoch 23/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 5.5709 - val_loss: 1.2522\n",
      "Epoch 24/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 7.3149 - val_loss: 10.5655\n",
      "Epoch 25/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 11.7361 - val_loss: 0.6627\n",
      "Epoch 26/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 2.8634 - val_loss: 2.9891\n",
      "Epoch 27/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 6.6198 - val_loss: 1.3768\n",
      "Epoch 28/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 932us/step - loss: 1.5499 - val_loss: 3.6446\n",
      "Epoch 29/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 4.8944 - val_loss: 0.6983\n",
      "Epoch 30/30\n",
      "\u001b[1m413/413\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.1348 - val_loss: 1.5187\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "# Define input shape\n",
    "input_shape = X_train.shape[1:]\n",
    "\n",
    "# Model A uses the He initialization and the ReLU activation function\n",
    "model_A = keras.models.Sequential([\n",
    "    keras.layers.Normalization(input_shape=input_shape),\n",
    "    keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "])\n",
    "\n",
    "# Model B's uses the default kernel initializer (Glorot) and no activation function\n",
    "model_B = keras.models.Sequential([\n",
    "    keras.layers.Normalization(input_shape=input_shape),\n",
    "    keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# Compile models A and B with the same loss and optimizer\n",
    "model_A.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "model_B.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "# Train for 30 epochs\n",
    "history_A = model_A.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "history_B = model_B.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 974us/step - loss: 6.0919\n",
      "Model A MSE:  6.383603096008301\n",
      "\u001b[1m104/104\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971us/step - loss: 1.4575\n",
      "Model B MSE:  1.5186644792556763\n"
     ]
    }
   ],
   "source": [
    "# Report the MSE on the validation set\n",
    "print(\"Model A MSE: \", model_A.evaluate(X_valid, y_valid))\n",
    "print(\"Model B MSE: \", model_B.evaluate(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [6 marks]\n",
    "\n",
    "Read the section \"Fine-Tuning Neural Network Hyperparameters\" in the textbook and the corresponding section in the [Jupyter notebook](https://github.com/ageron/handson-ml3/blob/main/10_neural_nets_with_keras.ipynb) on the textbook website using Keras Tuner. You will need to install the package `keras_tuner` if you don't already have it.\n",
    "\n",
    "Then use Keras Tuner to do a randomized search to search for the best hyperparameters. Do the randomized search over the first 5000 observations of the training set. Use 20 iterations, 20 epochs per iteration. Use the same network architecture as (a) except where otherwise specified below. Use no activation function for the output layer. Use a seed of 42, and the objective is clearly to minimize validation loss. The hyperparameters to search over are:\n",
    "\n",
    "* Hidden layers: 1 to 5.\n",
    "* Number of neurons per layer: 1 to 100.\n",
    "* Learning rate: 1e-4 to 1e-2 using log sampling.\n",
    "* $\\ell_2$ regularizers with `l2` value: 1e-4 to 100 using log sampling.\n",
    "* Optimizer: `tf.keras.optimizers.SGD(learning_rate=learning_rate,clipnorm=1.0)` and `tf.keras.optimizers.Nadam(learning_rate=learning_rate)`.\n",
    "\n",
    "Print the best hyperparameter. (You can ignore any warning message you may get)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 13s]\n",
      "val_loss: 32.508331298828125\n",
      "\n",
      "Best val_loss So Far: 1.081332802772522\n",
      "Total elapsed time: 00h 01m 07s\n",
      "\n",
      "Search: Running Trial #6\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "1                 |5                 |hidden_layers\n",
      "75                |62                |neurons\n",
      "0.002336          |0.0067187         |learning_rate\n",
      "28.749            |0.00034837        |l2\n",
      "SGD               |Nadam             |optimizer\n",
      "\n",
      "Epoch 1/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 813180.0000 - val_loss: 4906.6768\n",
      "Epoch 2/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4742.1826 - val_loss: 4633.2329\n",
      "Epoch 3/20\n",
      "\u001b[1m157/157\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 4566.0479 - val_loss: 4507.1138\n",
      "Epoch 4/20\n",
      "\u001b[1m 46/157\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 4544.9346 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 56\u001b[0m\n\u001b[0;32m     45\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[0;32m     46\u001b[0m     model_builder, \n\u001b[0;32m     47\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     53\u001b[0m )\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Search for the best hyperparameters\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch(X_train[:\u001b[38;5;241m5000\u001b[39m], y_train[:\u001b[38;5;241m5000\u001b[39m], epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, validation_data\u001b[38;5;241m=\u001b[39m(X_valid, y_valid))\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Supress warnings\u001b[39;00m\n\u001b[0;32m     59\u001b[0m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mset_verbosity(tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39mERROR)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m tracing_compilation\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    879\u001b[0m     args, kwds, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config\n\u001b[0;32m    880\u001b[0m )\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m function\u001b[38;5;241m.\u001b[39m_call_flat(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[38;5;241m=\u001b[39mfunction\u001b[38;5;241m.\u001b[39mcaptured_inputs\n\u001b[0;32m    141\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inference_function\u001b[38;5;241m.\u001b[39mcall_preflattened(args)\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_flat(\u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mcall_function(\n\u001b[0;32m    252\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mflat_outputs),\n\u001b[0;32m    255\u001b[0m     )\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute(\n\u001b[0;32m   1501\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1502\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   1503\u001b[0m       inputs\u001b[38;5;241m=\u001b[39mtensor_inputs,\n\u001b[0;32m   1504\u001b[0m       attrs\u001b[38;5;241m=\u001b[39mattrs,\n\u001b[0;32m   1505\u001b[0m       ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1506\u001b[0m   )\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import keras_tuner as kt\n",
    "\n",
    "\n",
    "def model_builder(hp):\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Sequential()    \n",
    "\n",
    "    # Input layer\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    model.add(keras.layers.Normalization())\n",
    "\n",
    "    # Hyperparameters\n",
    "    hp_hidden_layers = hp.Int(\"hidden_layers\", min_value=1, max_value=5)\n",
    "    hp_neurons = hp.Int(\"neurons\", min_value=1, max_value=100)\n",
    "    hp_learning_rate = hp.Float(\"learning_rate\", min_value=1e-4, max_value=1e-2, sampling=\"log\")\n",
    "    hp_l2 = hp.Float(\"l2\", min_value=1e-4, max_value=100, sampling=\"log\")\n",
    "\n",
    "\n",
    "    # Add hidden layers\n",
    "    for i in range(hp_hidden_layers):\n",
    "        model.add(\n",
    "            keras.layers.Dense(\n",
    "                hp_neurons,\n",
    "                activation=\"relu\",\n",
    "                kernel_initializer=\"he_normal\",\n",
    "                kernel_regularizer=keras.regularizers.l2(hp_l2)\n",
    "            ))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(keras.layers.Dense(1, kernel_regularizer=keras.regularizers.l2(hp_l2)))\n",
    "\n",
    "    # Set the optimizer\n",
    "    hp_optimizer = hp.Choice(\"optimizer\", [\"SGD\", \"Nadam\"])\n",
    "    if hp_optimizer == \"SGD\":\n",
    "        optimizer = tf.keras.optimizers.SGD(learning_rate=hp_learning_rate, clipnorm=1.0)\n",
    "    else:\n",
    "        optimizer = tf.keras.optimizers.Nadam(learning_rate=hp_learning_rate)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model\n",
    "\n",
    "# Create the tuner \n",
    "tuner = kt.RandomSearch(\n",
    "    model_builder, \n",
    "    objective=\"val_loss\",\n",
    "    directory=\"my_dir\",\n",
    "    project_name=\"HW4_Part1\",\n",
    "    seed=42,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(X_train[:5000], y_train[:5000], epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3472 - val_loss: 1.3155\n",
      "Epoch 2/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3559 - val_loss: 1.3146\n",
      "Epoch 3/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3434 - val_loss: 1.3126\n",
      "Epoch 4/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3459 - val_loss: 1.3132\n",
      "Epoch 5/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3373 - val_loss: 1.3194\n",
      "Epoch 6/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3233 - val_loss: 1.3119\n",
      "Epoch 7/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3389 - val_loss: 1.3148\n",
      "Epoch 8/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3333 - val_loss: 1.3143\n",
      "Epoch 9/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3337 - val_loss: 1.3146\n",
      "Epoch 10/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3474 - val_loss: 1.3191\n",
      "Epoch 11/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3399 - val_loss: 1.3113\n",
      "Epoch 12/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3123 - val_loss: 1.3132\n",
      "Epoch 13/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3324 - val_loss: 1.3111\n",
      "Epoch 14/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3307 - val_loss: 1.3117\n",
      "Epoch 15/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3125 - val_loss: 1.3127\n",
      "Epoch 16/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3512 - val_loss: 1.3125\n",
      "Epoch 17/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3676 - val_loss: 1.3114\n",
      "Epoch 18/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3142 - val_loss: 1.3114\n",
      "Epoch 19/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.2859 - val_loss: 1.3124\n",
      "Epoch 20/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3281 - val_loss: 1.3111\n",
      "Best Hyperparameters:\n",
      "{'hidden_layers': 3, 'neurons': 6, 'learning_rate': 0.008347597753558379, 'l2': 25.139076232316345, 'optimizer': 'Nadam'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "best_model.fit(X_train_tmp, y_train_tmp, epochs=20, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters:\\n{best_hps.values}\")\n",
    "\n",
    "# Plot the learning curves\n",
    "plt.plot(best_model.history.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(best_model.history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [2 marks]\n",
    "\n",
    "For the best model in (b), train the model on the full training data for 200 epochs. Plot the learning curve. Does it look like the model is overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\saving\\saving_lib.py:415: UserWarning: Skipping variable loading for optimizer 'nadam', because it has 2 variables whereas the saved optimizer has 19 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - loss: 1.3639 - val_loss: 1.3106\n",
      "Epoch 2/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3038 - val_loss: 1.3145\n",
      "Epoch 3/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3090 - val_loss: 1.3122\n",
      "Epoch 4/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3369 - val_loss: 1.3142\n",
      "Epoch 5/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3387 - val_loss: 1.3145\n",
      "Epoch 6/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3216 - val_loss: 1.3145\n",
      "Epoch 7/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - loss: 1.3276 - val_loss: 1.3129\n",
      "Epoch 8/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3454 - val_loss: 1.3119\n",
      "Epoch 9/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3321 - val_loss: 1.3151\n",
      "Epoch 10/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3260 - val_loss: 1.3127\n",
      "Epoch 11/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3487 - val_loss: 1.3126\n",
      "Epoch 12/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3487 - val_loss: 1.3120\n",
      "Epoch 13/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3315 - val_loss: 1.3131\n",
      "Epoch 14/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3172 - val_loss: 1.3148\n",
      "Epoch 15/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3282 - val_loss: 1.3124\n",
      "Epoch 16/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3221 - val_loss: 1.3136\n",
      "Epoch 17/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3559 - val_loss: 1.3120\n",
      "Epoch 18/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3345 - val_loss: 1.3123\n",
      "Epoch 19/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3425 - val_loss: 1.3114\n",
      "Epoch 20/20\n",
      "\u001b[1m516/516\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 1.3263 - val_loss: 1.3111\n",
      "Best Hyperparameters:\n",
      "{'hidden_layers': 3, 'neurons': 6, 'learning_rate': 0.008347597753558379, 'l2': 25.139076232316345, 'optimizer': 'Nadam'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train the best model\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n",
    "history = best_model.fit(X_train_tmp, y_train_tmp, epochs=200, validation_data=(X_test, y_test))\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"Best Hyperparameters:\\n{best_hps.values}\")\n",
    "\n",
    "# Plot the learning curves\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"])\n",
    "plt.show()\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary classification DNN [17 marks]\n",
    "\n",
    "Consider the [Portuguese Bank Marketing Data Set](https://www.kaggle.com/yufengsui/portuguese-bank-marketing-data-set?select=bank_cleaned.csv) available at Kaggle. Download the `bank_cleaned.csv` file or from [Canvas](https://canvas.uw.edu/files/106328167/download?download_frd=1). Here we want to predict the success or failure of a bank marketing campaign using phone calls to promote a term deposit product. The target variable is `response_binary`.\n",
    "\n",
    "The following code preprocesses the data. The day and month have been converted into cyclical features(1st day of the month has equal distance to the 2nd and the 31st)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryanm\\AppData\\Local\\Temp\\ipykernel_9324\\648047190.py:6: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  month_rad = (df[\"month\"].replace(month_dict) - 1) * (2 * np.pi / 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>balance</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>duration</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>response_binary</th>\n",
       "      <th>day_sin</th>\n",
       "      <th>day_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>58</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>2143</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>4.35</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>technician</td>\n",
       "      <td>single</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>29</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.52</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33</td>\n",
       "      <td>entrepreneur</td>\n",
       "      <td>married</td>\n",
       "      <td>secondary</td>\n",
       "      <td>no</td>\n",
       "      <td>2</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>1.27</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35</td>\n",
       "      <td>management</td>\n",
       "      <td>married</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>231</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>2.32</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>management</td>\n",
       "      <td>single</td>\n",
       "      <td>tertiary</td>\n",
       "      <td>no</td>\n",
       "      <td>447</td>\n",
       "      <td>yes</td>\n",
       "      <td>yes</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0</td>\n",
       "      <td>unknown</td>\n",
       "      <td>0</td>\n",
       "      <td>0.724793</td>\n",
       "      <td>0.688967</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age           job  marital  education default  balance housing loan  \\\n",
       "0   58    management  married   tertiary      no     2143     yes   no   \n",
       "1   44    technician   single  secondary      no       29     yes   no   \n",
       "2   33  entrepreneur  married  secondary      no        2     yes  yes   \n",
       "3   35    management  married   tertiary      no      231     yes   no   \n",
       "4   28    management   single   tertiary      no      447     yes  yes   \n",
       "\n",
       "   duration  campaign  pdays  previous poutcome  response_binary   day_sin  \\\n",
       "0      4.35         1     -1         0  unknown                0  0.724793   \n",
       "1      2.52         1     -1         0  unknown                0  0.724793   \n",
       "2      1.27         1     -1         0  unknown                0  0.724793   \n",
       "3      2.32         1     -1         0  unknown                0  0.724793   \n",
       "4      3.62         1     -1         0  unknown                0  0.724793   \n",
       "\n",
       "    day_cos  month_sin  month_cos  \n",
       "0  0.688967   0.866025       -0.5  \n",
       "1  0.688967   0.866025       -0.5  \n",
       "2  0.688967   0.866025       -0.5  \n",
       "3  0.688967   0.866025       -0.5  \n",
       "4  0.688967   0.866025       -0.5  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"bank_cleaned.csv\")\n",
    "\n",
    "month_dict = {\"jan\": 1, \"feb\": 2, \"mar\": 3, \"apr\": 4, \"may\": 5, \"jun\": 6,\n",
    "              \"jul\": 7, \"aug\": 8, \"sep\": 9, \"oct\": 10, \"nov\": 11, \"dec\": 12}\n",
    "day_rad = (df[\"day\"] - 1) * (2 * np.pi / 31)\n",
    "month_rad = (df[\"month\"].replace(month_dict) - 1) * (2 * np.pi / 12)\n",
    "df[\"day_sin\"] = np.sin(day_rad)\n",
    "df[\"day_cos\"] = np.cos(day_rad)\n",
    "df[\"month_sin\"] = np.sin(month_rad)\n",
    "df[\"month_cos\"]  = np.cos(month_rad)\n",
    "df.drop(columns=[\"Unnamed: 0\", \"month\", \"day\", \"response\"], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "train_set_tmp, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_set, valid_set = train_test_split(train_set_tmp, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_raw = train_set.drop(\"response_binary\", axis=1).copy()\n",
    "y_train = train_set[\"response_binary\"].copy()\n",
    "X_valid_raw = valid_set.drop(\"response_binary\", axis=1).copy()\n",
    "y_valid = valid_set[\"response_binary\"].copy()\n",
    "X_test_raw = test_set.drop(\"response_binary\", axis=1).copy()\n",
    "y_test = test_set[\"response_binary\"].copy()\n",
    "\n",
    "num_attribs = list(X_train_raw._get_numeric_data().columns)\n",
    "cat_attribs = list(set(X_train_raw.columns) - set(num_attribs))\n",
    "\n",
    "cat_attribs_ord = ['default', 'housing', 'loan']\n",
    "cat_attribs_hot = ['job', 'marital', 'education', 'poutcome']\n",
    "\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), num_attribs),\n",
    "        (\"cat_hot\", OneHotEncoder(), cat_attribs_hot),\n",
    "        (\"cat_ord\", OrdinalEncoder(categories=[['no','yes'],['no','yes'],['no','yes']]), cat_attribs_ord)\n",
    "    ])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(X_train_raw)\n",
    "X_valid = full_pipeline.transform(X_valid_raw)\n",
    "X_test = full_pipeline.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [4 marks]\n",
    "\n",
    "In the next part you will build and fit a DNN with 4 hidden layers of 100 neurons each. Use the following specifications:\n",
    "\n",
    "(i) He initialization and the Swish activation function.\n",
    "\n",
    "(ii) The output layer has 1 neuron with sigmoid activation.\n",
    "\n",
    "(iii) Compile with `loss=\"binary_crossentropy\"` and  `metrics=[\"AUC\"]`.\n",
    "\n",
    "Explain why the choices (i), (ii), and (iii) are justified.\n",
    "\n",
    "Also, state the proportion of sucesses in the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**\n",
    "\n",
    "**(i)** The Swish activation function is a great choice for complex tasks like predicting the success or failure from a market campaign. He Initialization is helpful when training models with mutiple layers as the outputs of each layer are typically not standardized, the exception is SELU, but this is helpful for Swish. \n",
    "\n",
    "**(ii)** The output layer with a single sigmoid activation is resonable for a binary classification problem. \n",
    "\n",
    "**(iii)** Again, binary crossentrophy is a reasonable function given that our target is a boolean value. AUC is a useful metric, however, this parameter is only used to gauage the performance of the model, for binary classification it is a good choice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - AUC: 0.8628 - loss: 0.2523 - val_AUC: 0.8995 - val_loss: 0.2269\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9102 - loss: 0.2140 - val_AUC: 0.9054 - val_loss: 0.2181\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9124 - loss: 0.2110 - val_AUC: 0.9048 - val_loss: 0.2232\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9175 - loss: 0.2097 - val_AUC: 0.9157 - val_loss: 0.2103\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9272 - loss: 0.1943 - val_AUC: 0.9166 - val_loss: 0.2136\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9272 - loss: 0.1990 - val_AUC: 0.9132 - val_loss: 0.2081\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9291 - loss: 0.1956 - val_AUC: 0.9144 - val_loss: 0.2138\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9292 - loss: 0.1962 - val_AUC: 0.9085 - val_loss: 0.2202\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9375 - loss: 0.1845 - val_AUC: 0.9069 - val_loss: 0.2214\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9388 - loss: 0.1834 - val_AUC: 0.9158 - val_loss: 0.2109\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9380 - loss: 0.1829 - val_AUC: 0.9012 - val_loss: 0.2287\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9406 - loss: 0.1784 - val_AUC: 0.9072 - val_loss: 0.2217\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9436 - loss: 0.1737 - val_AUC: 0.9067 - val_loss: 0.2264\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9420 - loss: 0.1756 - val_AUC: 0.9019 - val_loss: 0.2410\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9460 - loss: 0.1670 - val_AUC: 0.8973 - val_loss: 0.2477\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9454 - loss: 0.1649 - val_AUC: 0.8991 - val_loss: 0.2667\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9445 - loss: 0.1701 - val_AUC: 0.9027 - val_loss: 0.2561\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9512 - loss: 0.1626 - val_AUC: 0.8966 - val_loss: 0.2528\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9540 - loss: 0.1550 - val_AUC: 0.8952 - val_loss: 0.2523\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9575 - loss: 0.1480 - val_AUC: 0.8905 - val_loss: 0.2662\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step\n",
      "Proportion of successes in the training data:  0.9444848299345755\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(keras.layers.InputLayer(input_shape=X_train.shape[1:]))\n",
    "model.add(keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"))\n",
    "model.add(keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"))\n",
    "\n",
    "# Output layer\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", metrics=[\"AUC\"])\n",
    "\n",
    "# Fit the model\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Calculate the proportion of successes in the training\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_train_pred_binary = (y_train_pred > 0.5)\n",
    "\n",
    "# Report the proportion of successes in the training data\n",
    "print(\n",
    "    \"Proportion of successes in the training data: \",\n",
    "    np.mean(y_train_pred_binary == y_train.values.reshape(-1, 1))\n",
    ")\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "\n",
    "Train the model in (a) for 30 epochs and use exponential scheduling using the function below (`lr0=0.01`, `s=20`) and the NAG optimizer with `momentum=0.9`. Use a learning curve to comment on whether it is overfitting.\n",
    "\n",
    "At the start of fitting your model, run `reset_session()` given by the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution** The Model is under fitting to the data by the charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - AUC: 0.9689 - loss: 0.1317 - val_AUC: 0.8335 - val_loss: 0.3592\n",
      "Epoch 2/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9694 - loss: 0.1315 - val_AUC: 0.8359 - val_loss: 0.3663\n",
      "Epoch 3/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9718 - loss: 0.1257 - val_AUC: 0.8285 - val_loss: 0.3717\n",
      "Epoch 4/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - AUC: 0.9736 - loss: 0.1221 - val_AUC: 0.8383 - val_loss: 0.3648\n",
      "Epoch 5/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9761 - loss: 0.1145 - val_AUC: 0.8262 - val_loss: 0.3849\n",
      "Epoch 6/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9755 - loss: 0.1158 - val_AUC: 0.8303 - val_loss: 0.3993\n",
      "Epoch 7/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9738 - loss: 0.1192 - val_AUC: 0.8416 - val_loss: 0.3763\n",
      "Epoch 8/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9788 - loss: 0.1092 - val_AUC: 0.8300 - val_loss: 0.3948\n",
      "Epoch 9/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9800 - loss: 0.1027 - val_AUC: 0.8213 - val_loss: 0.4219\n",
      "Epoch 10/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9796 - loss: 0.1032 - val_AUC: 0.8112 - val_loss: 0.4263\n",
      "Epoch 11/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9800 - loss: 0.1031 - val_AUC: 0.8258 - val_loss: 0.4119\n",
      "Epoch 12/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9815 - loss: 0.1011 - val_AUC: 0.8153 - val_loss: 0.4189\n",
      "Epoch 13/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9797 - loss: 0.1011 - val_AUC: 0.8251 - val_loss: 0.4188\n",
      "Epoch 14/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9801 - loss: 0.1036 - val_AUC: 0.8195 - val_loss: 0.4166\n",
      "Epoch 15/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9848 - loss: 0.0908 - val_AUC: 0.8301 - val_loss: 0.4174\n",
      "Epoch 16/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9843 - loss: 0.0907 - val_AUC: 0.8109 - val_loss: 0.4534\n",
      "Epoch 17/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9843 - loss: 0.0915 - val_AUC: 0.8192 - val_loss: 0.4507\n",
      "Epoch 18/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9807 - loss: 0.0971 - val_AUC: 0.8209 - val_loss: 0.4234\n",
      "Epoch 19/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9848 - loss: 0.0905 - val_AUC: 0.8155 - val_loss: 0.4406\n",
      "Epoch 20/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9848 - loss: 0.0855 - val_AUC: 0.8166 - val_loss: 0.4371\n",
      "Epoch 21/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - AUC: 0.9864 - loss: 0.0832 - val_AUC: 0.8266 - val_loss: 0.4405\n",
      "Epoch 22/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9857 - loss: 0.0857 - val_AUC: 0.8220 - val_loss: 0.4491\n",
      "Epoch 23/30\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9853 - loss: 0.0841 - val_AUC: 0.8117 - val_loss: 0.4722\n",
      "Epoch 24/30\n",
      "\u001b[1m 10/817\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - AUC: 0.9930 - loss: 0.0698  "
     ]
    }
   ],
   "source": [
    "def reset_session(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "def exponential_decay(lr0, s):\n",
    "    return lambda epoch: lr0 * 0.1**(epoch / s)\n",
    "\n",
    "# Reset the session\n",
    "reset_session()\n",
    "\n",
    "# helpful function to build the optimizer\n",
    "def build_nag_optimizer(lr=0.01, momentum=0.9, nesterov=True):\n",
    "    learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate=lr,\n",
    "        decay_steps=20,\n",
    "        decay_rate=0.1\n",
    "    )\n",
    "    return keras.optimizers.SGD(learning_rate=lr, momentum=momentum, nesterov=nesterov)\n",
    "\n",
    "# Train the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "history = model.fit(X_train, y_train, epochs=30, validation_data=(X_valid, y_valid))\n",
    "\n",
    "# Plot the learning curves\n",
    "pd.DataFrame(history.history).plot(\n",
    "    figsize=(8, 5), grid=True, xlabel=\"Epoch\",\n",
    "    style=[\"r--\", \"r--.\", \"b-\", \"b-*\"]\n",
    ")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [8 marks]\n",
    "\n",
    "Fit separate models using the same specification as in (b) but with the following regularization techniques:\n",
    "\n",
    "(i) batch normalization,\n",
    "\n",
    "(ii) early stopping based on validation AUC with `patience=10` (look at the documentation and note the `mode` argument).\n",
    "\n",
    "(iii) $\\ell_2$ regularization with `l2=0.0002`,\n",
    "\n",
    "(iv) dropout with probability 0.02,\n",
    "\n",
    "(v) $\\ell_2$ regularization and early stopping both as above,\n",
    "\n",
    "(vi) batch normalization and dropout both as above.\n",
    "\n",
    "At the start of each one of the above models, run `reset_session()`.\n",
    "\n",
    "The performance measure is validation AUC. State this for the model in (b), and for each of the models here comment on whether it is better than the model in (b)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - AUC: 0.8153 - loss: 0.2880 - val_AUC: 0.8713 - val_loss: 0.2463\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9019 - loss: 0.2234 - val_AUC: 0.8751 - val_loss: 0.2439\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9169 - loss: 0.2090 - val_AUC: 0.8746 - val_loss: 0.2484\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9289 - loss: 0.1959 - val_AUC: 0.8661 - val_loss: 0.2599\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9394 - loss: 0.1830 - val_AUC: 0.8560 - val_loss: 0.2782\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9493 - loss: 0.1696 - val_AUC: 0.8428 - val_loss: 0.3041\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9575 - loss: 0.1561 - val_AUC: 0.8326 - val_loss: 0.3347\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9643 - loss: 0.1435 - val_AUC: 0.8219 - val_loss: 0.3748\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9718 - loss: 0.1288 - val_AUC: 0.8077 - val_loss: 0.4215\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9784 - loss: 0.1132 - val_AUC: 0.7985 - val_loss: 0.4782\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9810 - loss: 0.1075 - val_AUC: 0.7902 - val_loss: 0.4924\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9795 - loss: 0.1097 - val_AUC: 0.8000 - val_loss: 0.5042\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9851 - loss: 0.0958 - val_AUC: 0.7838 - val_loss: 0.5344\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9827 - loss: 0.1024 - val_AUC: 0.7980 - val_loss: 0.5289\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9871 - loss: 0.0864 - val_AUC: 0.7971 - val_loss: 0.5573\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9910 - loss: 0.0735 - val_AUC: 0.7650 - val_loss: 0.6298\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9888 - loss: 0.0822 - val_AUC: 0.7722 - val_loss: 0.6080\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9919 - loss: 0.0712 - val_AUC: 0.7649 - val_loss: 0.6330\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9909 - loss: 0.0688 - val_AUC: 0.7679 - val_loss: 0.6560\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9936 - loss: 0.0619 - val_AUC: 0.7840 - val_loss: 0.6370\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the models\n",
    "models = dict()\n",
    "\n",
    "# Model from b is labeled as model 0\n",
    "models['B'] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build Model 1\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Complie model 1 and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "models[1] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - AUC: 0.8339 - loss: 0.2781 - val_AUC: 0.8977 - val_loss: 0.2277\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9082 - loss: 0.2172 - val_AUC: 0.9042 - val_loss: 0.2217\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9153 - loss: 0.2107 - val_AUC: 0.9080 - val_loss: 0.2181\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9196 - loss: 0.2063 - val_AUC: 0.9099 - val_loss: 0.2157\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9229 - loss: 0.2029 - val_AUC: 0.9115 - val_loss: 0.2141\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9252 - loss: 0.2002 - val_AUC: 0.9125 - val_loss: 0.2129\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9274 - loss: 0.1978 - val_AUC: 0.9131 - val_loss: 0.2122\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9294 - loss: 0.1956 - val_AUC: 0.9136 - val_loss: 0.2118\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9312 - loss: 0.1936 - val_AUC: 0.9137 - val_loss: 0.2119\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9327 - loss: 0.1916 - val_AUC: 0.9137 - val_loss: 0.2123\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9344 - loss: 0.1896 - val_AUC: 0.9133 - val_loss: 0.2130\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9358 - loss: 0.1877 - val_AUC: 0.9129 - val_loss: 0.2139\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9372 - loss: 0.1857 - val_AUC: 0.9127 - val_loss: 0.2150\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9384 - loss: 0.1838 - val_AUC: 0.9120 - val_loss: 0.2164\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9396 - loss: 0.1818 - val_AUC: 0.9115 - val_loss: 0.2180\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9409 - loss: 0.1798 - val_AUC: 0.9103 - val_loss: 0.2199\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9423 - loss: 0.1778 - val_AUC: 0.9096 - val_loss: 0.2221\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9437 - loss: 0.1757 - val_AUC: 0.9086 - val_loss: 0.2245\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9453 - loss: 0.1735 - val_AUC: 0.9078 - val_loss: 0.2273\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9467 - loss: 0.1712 - val_AUC: 0.9064 - val_loss: 0.2304\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Model 2 with early stopping\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Set the early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10, mode=\"max\", monitor=\"val_auc\"\n",
    ")\n",
    "\n",
    "# Complie model 2 and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[early_stopping])\n",
    "models[2] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - AUC: 0.8230 - loss: 0.4352 - val_AUC: 0.8963 - val_loss: 0.3771\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9075 - loss: 0.3669 - val_AUC: 0.9029 - val_loss: 0.3631\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9128 - loss: 0.3529 - val_AUC: 0.9067 - val_loss: 0.3518\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9164 - loss: 0.3416 - val_AUC: 0.9090 - val_loss: 0.3422\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9186 - loss: 0.3319 - val_AUC: 0.9102 - val_loss: 0.3338\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - AUC: 0.9202 - loss: 0.3232 - val_AUC: 0.9114 - val_loss: 0.3263\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9218 - loss: 0.3153 - val_AUC: 0.9122 - val_loss: 0.3195\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9230 - loss: 0.3082 - val_AUC: 0.9130 - val_loss: 0.3133\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9239 - loss: 0.3016 - val_AUC: 0.9133 - val_loss: 0.3075\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9249 - loss: 0.2955 - val_AUC: 0.9137 - val_loss: 0.3022\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9257 - loss: 0.2899 - val_AUC: 0.9141 - val_loss: 0.2972\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9264 - loss: 0.2847 - val_AUC: 0.9145 - val_loss: 0.2926\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9270 - loss: 0.2799 - val_AUC: 0.9148 - val_loss: 0.2884\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9275 - loss: 0.2754 - val_AUC: 0.9150 - val_loss: 0.2844\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9279 - loss: 0.2713 - val_AUC: 0.9149 - val_loss: 0.2807\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9283 - loss: 0.2674 - val_AUC: 0.9152 - val_loss: 0.2773\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9284 - loss: 0.2638 - val_AUC: 0.9153 - val_loss: 0.2741\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9286 - loss: 0.2604 - val_AUC: 0.9156 - val_loss: 0.2711\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9289 - loss: 0.2572 - val_AUC: 0.9157 - val_loss: 0.2684\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9291 - loss: 0.2542 - val_AUC: 0.9159 - val_loss: 0.2658\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the models\n",
    "models = dict()\n",
    "\n",
    "# Build Model 3 with l2 regularization on the hidden layers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Complie model 1 and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "models[3] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - AUC: 0.8180 - loss: 0.2804 - val_AUC: 0.8936 - val_loss: 0.2302\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.8981 - loss: 0.2257 - val_AUC: 0.8994 - val_loss: 0.2250\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - AUC: 0.9037 - loss: 0.2213 - val_AUC: 0.9033 - val_loss: 0.2213\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9083 - loss: 0.2162 - val_AUC: 0.9059 - val_loss: 0.2195\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9095 - loss: 0.2149 - val_AUC: 0.9101 - val_loss: 0.2160\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9123 - loss: 0.2128 - val_AUC: 0.9111 - val_loss: 0.2149\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9162 - loss: 0.2086 - val_AUC: 0.9125 - val_loss: 0.2127\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9186 - loss: 0.2066 - val_AUC: 0.9136 - val_loss: 0.2122\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9197 - loss: 0.2054 - val_AUC: 0.9159 - val_loss: 0.2104\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9195 - loss: 0.2054 - val_AUC: 0.9159 - val_loss: 0.2092\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9211 - loss: 0.2040 - val_AUC: 0.9147 - val_loss: 0.2102\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9235 - loss: 0.2012 - val_AUC: 0.9157 - val_loss: 0.2097\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9265 - loss: 0.1981 - val_AUC: 0.9166 - val_loss: 0.2091\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9264 - loss: 0.1981 - val_AUC: 0.9159 - val_loss: 0.2089\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9271 - loss: 0.1968 - val_AUC: 0.9172 - val_loss: 0.2073\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9278 - loss: 0.1963 - val_AUC: 0.9171 - val_loss: 0.2084\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - AUC: 0.9301 - loss: 0.1940 - val_AUC: 0.9180 - val_loss: 0.2079\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9315 - loss: 0.1922 - val_AUC: 0.9166 - val_loss: 0.2093\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9318 - loss: 0.1916 - val_AUC: 0.9163 - val_loss: 0.2091\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9317 - loss: 0.1909 - val_AUC: 0.9173 - val_loss: 0.2080\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the models\n",
    "models = dict()\n",
    "\n",
    "# Build Model 4 with Dropout of 0.02\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Complie model 4 and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "models[4] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - AUC: 0.8115 - loss: 0.4461 - val_AUC: 0.8971 - val_loss: 0.3789\n",
      "Epoch 2/20\n",
      "\u001b[1m 42/817\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9260 - loss: 0.3486"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\callbacks\\early_stopping.py:155: UserWarning: Early stopping conditioned on metric `val_auc` which is not available. Available metrics are: AUC,loss,val_AUC,val_loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9061 - loss: 0.3686 - val_AUC: 0.9013 - val_loss: 0.3658\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9104 - loss: 0.3554 - val_AUC: 0.9041 - val_loss: 0.3550\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9132 - loss: 0.3444 - val_AUC: 0.9059 - val_loss: 0.3454\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9157 - loss: 0.3347 - val_AUC: 0.9074 - val_loss: 0.3367\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9176 - loss: 0.3260 - val_AUC: 0.9086 - val_loss: 0.3287\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9191 - loss: 0.3180 - val_AUC: 0.9098 - val_loss: 0.3214\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9206 - loss: 0.3106 - val_AUC: 0.9110 - val_loss: 0.3146\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9217 - loss: 0.3038 - val_AUC: 0.9122 - val_loss: 0.3084\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9226 - loss: 0.2976 - val_AUC: 0.9131 - val_loss: 0.3027\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9236 - loss: 0.2918 - val_AUC: 0.9140 - val_loss: 0.2973\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9245 - loss: 0.2864 - val_AUC: 0.9146 - val_loss: 0.2924\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9253 - loss: 0.2814 - val_AUC: 0.9152 - val_loss: 0.2878\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9259 - loss: 0.2768 - val_AUC: 0.9158 - val_loss: 0.2835\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - AUC: 0.9267 - loss: 0.2724 - val_AUC: 0.9163 - val_loss: 0.2795\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9272 - loss: 0.2683 - val_AUC: 0.9167 - val_loss: 0.2758\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9279 - loss: 0.2645 - val_AUC: 0.9173 - val_loss: 0.2724\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - AUC: 0.9285 - loss: 0.2609 - val_AUC: 0.9177 - val_loss: 0.2692\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9291 - loss: 0.2575 - val_AUC: 0.9180 - val_loss: 0.2662\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9295 - loss: 0.2544 - val_AUC: 0.9182 - val_loss: 0.2635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build Model 5 with early stopping and l2 regularization on the hidden layers\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\", kernel_regularizer=keras.regularizers.l2(0.0002)),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Set the early stopping callback\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    patience=10, mode=\"max\", monitor=\"val_auc\"\n",
    ")\n",
    "\n",
    "# Complie model 2 and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid), callbacks=[early_stopping])\n",
    "models[5] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ryanm\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - AUC: 0.7898 - loss: 0.3303 - val_AUC: 0.8802 - val_loss: 0.2407\n",
      "Epoch 2/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.8888 - loss: 0.2351 - val_AUC: 0.8815 - val_loss: 0.2395\n",
      "Epoch 3/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - AUC: 0.9020 - loss: 0.2234 - val_AUC: 0.8836 - val_loss: 0.2359\n",
      "Epoch 4/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - AUC: 0.9081 - loss: 0.2161 - val_AUC: 0.8832 - val_loss: 0.2396\n",
      "Epoch 5/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9146 - loss: 0.2102 - val_AUC: 0.8794 - val_loss: 0.2422\n",
      "Epoch 6/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9237 - loss: 0.2011 - val_AUC: 0.8760 - val_loss: 0.2498\n",
      "Epoch 7/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9278 - loss: 0.1953 - val_AUC: 0.8731 - val_loss: 0.2521\n",
      "Epoch 8/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9345 - loss: 0.1876 - val_AUC: 0.8713 - val_loss: 0.2583\n",
      "Epoch 9/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9382 - loss: 0.1825 - val_AUC: 0.8711 - val_loss: 0.2658\n",
      "Epoch 10/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9413 - loss: 0.1790 - val_AUC: 0.8624 - val_loss: 0.2767\n",
      "Epoch 11/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9459 - loss: 0.1719 - val_AUC: 0.8590 - val_loss: 0.2790\n",
      "Epoch 12/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9513 - loss: 0.1630 - val_AUC: 0.8530 - val_loss: 0.2979\n",
      "Epoch 13/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9499 - loss: 0.1654 - val_AUC: 0.8548 - val_loss: 0.2963\n",
      "Epoch 14/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9553 - loss: 0.1566 - val_AUC: 0.8422 - val_loss: 0.3179\n",
      "Epoch 15/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9577 - loss: 0.1531 - val_AUC: 0.8437 - val_loss: 0.3202\n",
      "Epoch 16/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9624 - loss: 0.1457 - val_AUC: 0.8434 - val_loss: 0.3322\n",
      "Epoch 17/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9632 - loss: 0.1440 - val_AUC: 0.8445 - val_loss: 0.3419\n",
      "Epoch 18/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - AUC: 0.9645 - loss: 0.1404 - val_AUC: 0.8332 - val_loss: 0.3414\n",
      "Epoch 19/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - AUC: 0.9636 - loss: 0.1410 - val_AUC: 0.8386 - val_loss: 0.3459\n",
      "Epoch 20/20\n",
      "\u001b[1m817/817\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - AUC: 0.9654 - loss: 0.1346 - val_AUC: 0.8412 - val_loss: 0.3524\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to store the models\n",
    "models = dict()\n",
    "\n",
    "# Build Model with Batch Normalization and Dropout\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=X_train.shape[1:]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(100, activation=\"swish\", kernel_initializer=\"he_normal\"),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dropout(0.02),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Complie model and add it to the dictionary\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=build_nag_optimizer(), metrics=[\"AUC\"])\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "models[6] = model\n",
    "\n",
    "# Supress warnings\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Report the AUC metric for each model in models\n",
    "\n",
    "for i, model in models.items():\n",
    "    print(f\"Model {i} AUC: \", model.evaluate(X_valid, y_valid, return_dict=True)[\"auc\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [1 mark]\n",
    "\n",
    "For the dropout model in (c)(iv) determine whether or not it is overfitting less than the model in (b)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [1 mark]\n",
    "\n",
    "Of the models in (b) and (c), one would now choose the best model according to the performance metric (validation AUC) to evaluate on the test set. But instead, evaluate the model in (c)(v) on the test set in terms of the AUC and confusion matrix (regardless of whether it is the best model given your results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Time series using machine learning [14 marks]\n",
    "\n",
    "Obtain daily values of the [Japan/U.S. Foreign Exchange Rate (DEXJPUS)](https://fred.stlouisfed.org/series/DEXJPUS) starting from Jan 1, 1990, to Jan 1, 2023, from FRED. This can be obtained using the code below or you can download the data as a csv file from [Canvas](https://canvas.uw.edu/files/106328118/download?download_frd=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "data = pdr.get_data_fred('DEXJPUS', datetime(1990,1,1),datetime(2023,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [2 marks]\n",
    "\n",
    "Create a training set (before 2010), a validation set (Jan 2010 to Dec 2015), and a test set (the rest of the data). Turn the time series data into a supervised learning dataset where the features are the value of the exchange rate in the last 10 days inclusive of the current day, and the target is the value of the exchange rate in the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "\n",
    "Fit a random forest regressor to predict the value of the exchange rate in the next day. Using the test set, report the mean squared error and the accuracy for the movement direction.\n",
    "\n",
    "Hint: You can calculate the accuracy of the movement direction by determining what the actual movement direction is and comparing it to the movement direction corresponding to the predicted value of the exchange rate. For instance, the movement direction of the test set `X_test` and `y_test` where a strictly up movement is `True` can be computed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_test = X_test[:,-1] < y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [4 marks]\n",
    "\n",
    "Repeat (b), but now fit a deep RNN with 2 recurrent layers of 20 and 20 neurons, and an output layer which is 1 dense neuron. Use 100 epochs and the Nadam optimizer. Comment on the result and the learning curve (the validation set is used for the learning curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [5 marks]\n",
    "\n",
    "Create a supervised learning dataset suitable for predicting 3 days ahead instead of 1 day ahead. Adjust the deep RNN in (c) so that it predicts 3 days ahead. Use 100 epochs and the Nadam optimizer. Using the test set, report the mean squared error and the accuracy for the movement direction for each of the 3 days ahead predictions.  Comment on the result and the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
